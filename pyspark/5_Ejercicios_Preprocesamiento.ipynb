{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f97722e2",
      "metadata": {
        "id": "f97722e2"
      },
      "source": [
        "> NOMBRE: Elizon Frank Carcausto Mamani\n",
        "\n",
        "> CODIGO: 170427"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalacion e importacion de recursos"
      ],
      "metadata": {
        "id": "FL1E3_Yx96ia"
      },
      "id": "FL1E3_Yx96ia"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey7zNkr64xco",
        "outputId": "4a72eb03-d932-4456-c1a5-963657ee5150"
      },
      "id": "Ey7zNkr64xco",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 56.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=41092b896263646c945966c9476623c1bb965a642dc21da0a398bcc8c44284a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\").getOrCreate()"
      ],
      "metadata": {
        "id": "eSY0s4bO61f6"
      },
      "id": "eSY0s4bO61f6",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dccff7ed",
      "metadata": {
        "id": "dccff7ed"
      },
      "source": [
        "# 1. Similitud de Cosenos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"x\" y \"y\" son dos array RDD numericos \n",
        "def simili_cos(x,y):\n",
        "  def product_p(x,y):\n",
        "    # reduce: sumar los productos\n",
        "    # map: multiplicar valores de x[0] y x [1] de la tupla x\n",
        "    return x.zip(y).map(lambda x: x[0]*x[1]).reduce(lambda x , y :x+y) \n",
        "  return product_p(x,y)/(product_p(x,x)*product_p(y,y))\n",
        "\n",
        "sc =SparkContext.getOrCreate()\n",
        "x = sc.parallelize(range(0,25),5)\n",
        "y = sc.parallelize(range(50, 75),5)\n",
        "print(simili_cos(x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwoCOt-J-1Fw",
        "outputId": "bb0be30d-0b64-44bf-8fec-2e3897722438"
      },
      "id": "fwoCOt-J-1Fw",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1696349997904704e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2d40d87",
      "metadata": {
        "id": "f2d40d87"
      },
      "source": [
        "# 2. Estandarizacion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"x\" es un array RDD numerico\n",
        "def estandarizacion(x):\n",
        "  # calculamos la media y desviacion estandar del RDD\n",
        "  media, std  = x.mean(), x.stdev()\n",
        "  # Mapeamos cada valor de la lista para estandarizar\n",
        "  lista= x.map(lambda xi : (xi-media)/std)\n",
        "  return lista\n",
        "\n",
        "Vec = sc.parallelize(range(25,50),5)\n",
        "VectorEs=estandarizacion(Vec)\n",
        "print(VectorEs.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLidSdhNE8bW",
        "outputId": "0ba178fc-2c30-4a15-dcad-fe0aee2c53a4"
      },
      "id": "NLidSdhNE8bW",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.6641005886756874, -1.52542553961938, -1.386750490563073, -1.2480754415067656, -1.1094003924504583, -0.9707253433941511, -0.8320502943378437, -0.6933752452815365, -0.5547001962252291, -0.41602514716892186, -0.2773500981126146, -0.1386750490563073, 0.0, 0.1386750490563073, 0.2773500981126146, 0.41602514716892186, 0.5547001962252291, 0.6933752452815365, 0.8320502943378437, 0.9707253433941511, 1.1094003924504583, 1.2480754415067656, 1.386750490563073, 1.52542553961938, 1.6641005886756874]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c10c271",
      "metadata": {
        "id": "1c10c271"
      },
      "source": [
        "# 3. Escalonamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un data set para los dos siguiente ejemplos"
      ],
      "metadata": {
        "id": "icd0ddHiFygO"
      },
      "id": "icd0ddHiFygO"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "# Creamos un dataset de prueba\n",
        "dataFrame = spark.createDataFrame([\n",
        "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
        "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
        "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
        "], [\"id\", \"features\"])"
      ],
      "metadata": {
        "id": "QfZ1cqGaEjPn"
      },
      "id": "QfZ1cqGaEjPn",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "67d05a0d",
      "metadata": {
        "id": "67d05a0d",
        "outputId": "d5d28411-6282-492e-c7cd-693ce105b1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funciones escaladas al rango: [0.000000, 1.000000]\n",
            "+--------------+--------------+\n",
            "|      features|scaledFeatures|\n",
            "+--------------+--------------+\n",
            "|[1.0,0.1,-1.0]|     (3,[],[])|\n",
            "| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n",
            "|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n",
            "+--------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import MinMaxScaler\n",
        "\n",
        "# Configuramos el MinMaxScaler (importada de una libreria)\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# ajustamos el scaler para calcular las estadísticas resumidas\n",
        "scalerModel = scaler.fit(dataFrame)\n",
        "\n",
        "# Cambiar la escala de cada característica al rango [mínimo, máximo]\n",
        "scaledData = scalerModel.transform(dataFrame)\n",
        "print(\"Funciones escaladas al rango: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
        "scaledData.select(\"features\", \"scaledFeatures\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9001acf5",
      "metadata": {
        "id": "9001acf5"
      },
      "source": [
        "# 4. Normalizacion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "50b577c3",
      "metadata": {
        "id": "50b577c3",
        "outputId": "7327e0f9-fa39-4c59-dd30-5de3c043ca28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizando....\n",
            "+---+--------------+--------------------+\n",
            "| id|      features|        normFeatures|\n",
            "+---+--------------+--------------------+\n",
            "|  0|[1.0,0.1,-1.0]|[0.47619047619047...|\n",
            "|  1| [2.0,1.1,1.0]|[0.48780487804878...|\n",
            "|  2|[3.0,10.1,3.0]|[0.18633540372670...|\n",
            "+---+--------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import Normalizer\n",
        "\n",
        "# Normaliza cada vector usando la Normalizer.\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
        "l1NormData = normalizer.transform(dataFrame)\n",
        "print(\"Normalizando....\")\n",
        "l1NormData.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dba31a0",
      "metadata": {
        "id": "5dba31a0"
      },
      "source": [
        "# 5. Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9a39ca42",
      "metadata": {
        "id": "9a39ca42",
        "outputId": "7c0602ca-ffcf-46a1-d5a4-e5aa39cd7bee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data set\n",
            "+-----+-----+------+--------------------+\n",
            "|Title|Month|Author|            Document|\n",
            "+-----+-----+------+--------------------+\n",
            "|    A|  Jan|  John|don quijote salio...|\n",
            "|    B|  Feb|  Mary|mancha de don qui...|\n",
            "|    C|  Mar|  Luke|sancho su fiel es...|\n",
            "|    D|  Apr|  Mark|quijote y sancho,...|\n",
            "+-----+-----+------+--------------------+\n",
            "\n",
            " Bag of Word\n",
            "+--------+-----+\n",
            "|   lower|count|\n",
            "+--------+-----+\n",
            "|      en|    1|\n",
            "|     don|    2|\n",
            "|  cabalo|    1|\n",
            "|  sancho|    2|\n",
            "|    fiel|    1|\n",
            "|  mancha|    2|\n",
            "|      de|    2|\n",
            "|      el|    2|\n",
            "|      su|    1|\n",
            "| quijote|    3|\n",
            "|       y|    2|\n",
            "|escudero|    1|\n",
            "|    deja|    1|\n",
            "|      no|    1|\n",
            "|   salio|    1|\n",
            "|   burro|    1|\n",
            "|      la|    1|\n",
            "|      lo|    1|\n",
            "|     paz|    1|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importamos la librerías a utilizar\n",
        "from pyspark.sql.functions import explode, split, regexp_replace, col, lower\n",
        "# Data\n",
        "data = [\n",
        "    ['A', 'Jan', 'John', 'don quijote salio de la mancha'],\n",
        "    ['B', 'Feb', 'Mary', 'mancha de don quijote no lo deja en paz'],\n",
        "    ['C', 'Mar', 'Luke', 'sancho su fiel escudero'],\n",
        "    ['D', 'Apr', 'Mark', 'quijote y sancho, el cabalo y el burro']\n",
        "]\n",
        "columns = ['Title', 'Month', 'Author', 'Document']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "#Mostrando el df\n",
        "print(\" Data set\")\n",
        "df.show()\n",
        "\n",
        "#BOG\n",
        "print(\" Bag of Word\")\n",
        "df.select(explode(split(regexp_replace(\"Document\", \"[,.-]\", \" \"), \"\\s+\")).alias(\"word\"))\\\n",
        "    .groupby(lower(col(\"word\")).alias(\"lower\"))\\\n",
        "    .count()\\\n",
        "    .show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8rc1"
    },
    "colab": {
      "name": "5_Ejercicios_Preprocesamiento.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}